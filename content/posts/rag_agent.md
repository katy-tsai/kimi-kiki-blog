---
title: å¯¦ä½œ LangChain RAG Agentï¼šå¾é›¶æ‰“é€ å¥åº·é£²é£Ÿå•ç­”ç³»çµ±
excerpt: åˆ©ç”¨è¡›æœéƒ¨æ¯æ—¥é£²é£ŸæŒ‡å—æ‰‹å†Šæ‰“é€ å¥åº·é£²é£Ÿå•ç­”ç³»çµ±
date: 2025-10-28
tags: 
 - RAG
 - Agent
 - Python
 - LangChain
author: 
  name: kimi-kiki
  link: /images/avatar.jpg
featured: false
---
> RAG Agent çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ã€Œå…ˆå¾çŸ¥è­˜åº«æª¢ç´¢ï¼Œå†ç”Ÿæˆå›ç­”ã€ã€‚
é€éè®“ AI åœ¨å›ç­”å‰å…ˆå¾å¤–éƒ¨çŸ¥è­˜åº«æœå°‹ç›¸é—œå…§å®¹ï¼Œ
èƒ½è®“ç”Ÿæˆçš„ç­”æ¡ˆæ›´è²¼è¿‘çŸ¥è­˜æä¾›è€…æƒ³å‚³éçš„æ­£ç¢ºè³‡è¨Šï¼Œ
åŒæ™‚å¤§å¹…é™ä½æ¨¡å‹ç”¢ç”Ÿå¹»è¦ºï¼ˆHallucinationï¼‰çš„é¢¨éšªã€‚
## ä¸€ã€å‰è¨€
ç‚ºäº†å¯¦éš›ç·´ç¿’é€™é …æŠ€è¡“ï¼Œä»¥è¡›ç”Ÿç¦åˆ©éƒ¨æ¯æ—¥é£²é£ŸæŒ‡å—æ‰‹å†Š ç‚ºä¸»è¦çŸ¥è­˜ä¾†æºï¼Œå¯¦ä½œäº†ä¸€å€‹èƒ½å›ç­”å¥åº·é£²é£Ÿç›¸é—œå•é¡Œçš„ RAG Agent ç³»çµ±ï¼Œ
è®“ä½¿ç”¨è€…å¯ä»¥é€éè‡ªç„¶èªè¨€è©¢å•ï¼Œç²å¾—å¯é ä¸”ç¬¦åˆå®˜æ–¹å»ºè­°çš„é£²é£Ÿå»ºè­°ã€‚

## äºŒã€RAG æ¶æ§‹æ¦‚å¿µåœ–
```plantuml
@startuml
title RAGï¼ˆRetrieval-Augmented Generationï¼‰æ¶æ§‹æµç¨‹åœ–

actor ä½¿ç”¨è€… as User
node "RAG ç³»çµ±" as RAG {
  node "Retriever" as Retriever
  node "LLM (Generator)" as Generator
}
database "å‘é‡è³‡æ–™åº«\n(Vector Store)" as VectorDB
folder "æ–‡ä»¶è³‡æ–™ä¾†æº\n(PDF, Markdown, API)" as Docs

User --> RAG : æå‡ºå•é¡Œ
RAG --> Retriever : å°‡å•é¡Œè½‰æˆå‘é‡ä¸¦æª¢ç´¢
Retriever --> VectorDB : æœå°‹æœ€ç›¸é—œçš„å…§å®¹
VectorDB --> Retriever : å›å‚³ç›¸é—œæ–‡ä»¶ç‰‡æ®µ
Retriever --> Generator : å‚³å…¥æª¢ç´¢çµæœ + å•é¡Œ
Generator --> User : ç”Ÿæˆæ•´åˆæ€§å›ç­”
Docs --> VectorDB : è³‡æ–™é å…ˆåµŒå…¥ä¸¦å»ºç«‹ç´¢å¼•

@enduml

```

## ä¸‰ã€ç’°å¢ƒæº–å‚™
### 1. python ç’°å¢ƒ
```cmd
mkdir health_food_agent
cd health_food_agent

uv init -p 3.13
```
### 2.å®‰è£æ‰€éœ€å¥—ä»¶

```
uv add python-dotenv pypdf langgraph langchain-openai langchain-community langchain-text-splitters langchain-chroma
```

### 3. å°ˆæ¡ˆæ¶æ§‹
```
health_food_agent/
â”œâ”€â”€ ğŸ“ .venv/                 (Pythonè™›æ“¬ç’°å¢ƒ)
â”œâ”€â”€ ğŸ“ data/                  (æ•¸æ“šæ–‡ä»¶å¤¾)
        â”œâ”€â”€ health_food.pdf  (è¡›ç”Ÿç¦åˆ©éƒ¨æ¯æ—¥é£²é£ŸæŒ‡å—æ‰‹å†Š)     
â”œâ”€â”€ ğŸ“ vectorstore/           (å‘é‡å­˜å„²)
â”œâ”€â”€ ğŸ“„ .env                   (ç’°å¢ƒè®Šé‡é…ç½®)
â”œâ”€â”€ ğŸ“„ .gitignore             (Gitå¿½ç•¥æ–‡ä»¶)
â”œâ”€â”€ ğŸ“„ .python-version        (Pythonç‰ˆæœ¬æŒ‡å®š)
â”œâ”€â”€ ğŸ main.py                (ä¸»ç¨‹åºå…¥å£)
â”œâ”€â”€ ğŸ“‹ pyproject.toml         (é …ç›®é…ç½®æ–‡ä»¶ - uv/Poetry)
â””â”€â”€ ğŸ“– README.md              (é …ç›®èªªæ˜æ–‡æª”)

```
## Step1: import æ‰€éœ€å¥—ä»¶
- dotenv ç’°å¢ƒç›¸é—œåƒæ•¸è¨­å®šå¼•ç”¨
  ```python
    from dotenv import load_dotenv
    load_dotenv()
  ```
  > ä¸è«–ä½¿ç”¨ä½•ç¨®ç”Ÿæˆå¼æ¨¡å‹(LLM), éƒ½éœ€è¦æä¾› api_key , æœ€å¥½ä¸è¦ç›´æ¥å¯«åœ¨ç¨‹å¼ä¸­ï¼Œéœ€è¦è¦ä¸€å€‹.envçš„æª”æ¡ˆä¾†å­˜æ”¾ï¼Œè€Œåœ¨ç¨‹å¼ä¸­å¾dotenv å¥—ä»¶ä¸­å¼•ç”¨load_dotenv ï¼Œä¾†è®“ç¨‹å¼è‡ªå‹•å–å¾—api_key
  
- pdf è³‡æ–™é å…ˆåµŒå…¥ä¸¦å»ºç«‹ç´¢å¼•

    ```python
    ## os ç”¨ä¾†åˆ¤æ–·æª”æ¡ˆè·¯å¾‘æ˜¯å¦å­˜åœ¨
    import os
    ## PyPDFLoader è®€å–pdfå…§å®¹
    from langchain_community.document_loaders import PyPDFLoader
    ## RecursiveCharacterTextSplitter å°‡pdf å…§å®¹åˆ†æˆchucks
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    ##  OpenAIEmbeddings å°‡åˆ†å‰²æˆchcksçš„è³‡æ–™è½‰æˆå‘é‡
    from langchain_openai import  OpenAIEmbeddings
    ## Chroma å‘é‡è³‡æ–™åº«
    from langchain_chroma import Chroma
    ```

- langchain ç›¸é—œ
    ```python
    ## llmå°è©±æ¨¡å‹
    from langchain_openai import ChatOpenAI
    ## å°è©±æ¨¡å‹è¨Šæ¯ç¨®é¡
    from langchain_core.messages import BaseMessage,SystemMessage,ToolMessage,HumanMessage
    ## è¨»æ˜llmå°è©±æ¨¡å‹å¯ä½¿ç”¨å·¥å…·
    from langchain_core.tools import tool
    ```
- langGraph ç›¸é—œ
    ```python
    ## langgraph ç•«å‡ºstateGraph , å¯ä»¥å„²å­˜æµç¨‹çš„state
    from langgraph.graph import StateGraph,START,END
    ## typing ç”¨ä¾†è¦ç¯„å­˜å„²stateçš„é¡å‹
    from typing import TypedDict,Sequence,Annotated
    ## Annotated ä¸­ç”¨add_messagesä¾†å¢åŠ typeçš„metadata
    from langgraph.graph.message import add_messages
    ```

## Step 2 æ–‡ä»¶è³‡æ–™å‰è™•ç†
- è®€å–pdfå…§å®¹ (å°‡è³‡æ–™æ”¾åˆ°dataè³‡æ–™å¤¾ä¸‹)ï¼Œç”¨PyPDFLoader å–å¾—pdfè³‡æ–™
  ```python
    pdf_path="./data/health_food.pdf"
    ## load documents 
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")
    pdf_loader = PyPDFLoader(pdf_path)
    ## check pdf
    try:
        pages = pdf_loader.load()
        print(f"PDF loaded successfully with {len(pages)} pages.")

    except Exception as e:
        print(f"Error loading PDF: {e}")
        raise 
  ```

- ç”¨RecursiveCharacterTextSplitter æ‹†åˆ†chuck æ–‡æœ¬åˆ†æ®µï¼‰
    ```python
    ## split documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, ## æ¯å€‹ chunk å¤§å°ç‚º 1000 å­—å…ƒ
        chunk_overlap=200 ## é‡ç–Šéƒ¨åˆ†ç‚º 200 å­—å…ƒ
    )
    pages_split = text_splitter.split_documents(pages)
    print(f"Total chunks created: {len(pages_split)}")
    ```

## Step3 å»ºç«‹å‘é‡è³‡æ–™åº«
- ä½¿ç”¨ Embedding æ¨¡å‹è½‰æ›æ–‡æœ¬ â†’ å‘é‡
  ```python
    ## create vector store
    embeddings=OpenAIEmbeddings(model="text-embedding-3-small")

    persist_directory = "./vectorstore"
    collection_name = "health_food_collection"
    if not os.path.exists(persist_directory):
        os.makedirs(persist_directory)  
    try:
        vector_store = Chroma.from_documents(
            documents=pages_split,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
    except Exception as e:
        print(f"Error creating vector store: {e}")
        raise
  ```

## Step4: å®šç¾©æŸ¥è©¢å·¥å…·
  - å®šç¾©å¦‚ä½•å¾vector store æŸ¥è©¢çš„å·¥å…·
   ```python
    ## å®šç¾©å·¥å…·
    retriever = vector_store.as_retriever(
        search_type="similarity", 
        search_kwargs={"k": 3} ## è¿”å›æœ€ç›¸ä¼¼çš„ 5 å€‹æ–‡ä»¶
        )  
    @tool
    def retriever_tool(query: str) -> str:
        """Retrieve relevant documents based on the query."""
        docs = retriever.invoke(query)
        if not docs:
            return "No relevant documents found."
        results=[]
        for i, doc in enumerate(docs):
            print(f"Document {i+1}:\n{doc.page_content}")
            results.append(f"Document {i+1}:\n{doc.page_content}")
            
        return "\n\n".join(results)

    tools = [retriever_tool]
    llm=ChatOpenAI(
            model_name="gpt-4",
            temperature=0 ## è¨­å®šç‚º 0 ä»¥ç²å¾—æ›´æº–ç¢ºçš„å›ç­”
        )
    llm=llm.bind_tools(tools)

    tools_dict={tool.name:tool for tool in tools}

   ```
 > llm è¨­å®šå‰ï¼Œè¦è¨˜å¾—åœ¨.env æ–‡ä»¶è£¡è¨­å®š apy Key
 ```cmd
 OPENAI_API_KEY=sk-xxxxx.............
 ```
## step5: è«‹Ai æ ¹æ“špdf ç”¢å‡ºai agent retriever çš„ systeom prompt
- ä¸‹é¢æˆ‘æ˜¯ç”¨è¡›ç”Ÿç¦åˆ©éƒ¨æ¯æ—¥é£²é£ŸæŒ‡å—æ‰‹å†Š è«‹claude ä»¥ç²¾ç°¡çš„æ–¹å¼ ç”¢å‡ºai agent çš„ systeom prompt 
```python
    system_prompt="""
    You are a nutrition assistant based on Taiwanâ€™s Daily Dietary Guidelines.
    Answer in Traditional Chinese, citing handbook data.
    Tasks:
    Explain six food groups and daily portions
    Give serving conversions and nutrient info
    Use retrieval for exact data or examples

    Rules:
    Be specific with units (g, bowl, serving)
    Use common Taiwanese foods
    Suggest professional help for special diets
    Promote balanced six-group intake
    """
```
## step6: å»ºç«‹LangGraph çš„stateGraph
![stateGraph](/public/images/20251028-165843.png)

```python
    ## 1. å®šç¾© State
    class AgentState(TypedDict):
        messages: Annotated[Sequence[BaseMessage],add_messages]

    ## 2. å®šç¾©node -llm action
    def call_llm(state: AgentState) -> AgentState:
        messages=list(state["messages"])
        messages=[SystemMessage(content=system_prompt)]+messages
        message=llm.invoke(messages)
        return {"messages":[message]}

    ## 3. å®šç¾©node -retriver_agent action
    def retriver_action(state:AgentState)->AgentState:
        tool_calls=state["messages"][-1].tool_calls
        results=[]
        for t in tool_calls:
            print(f"calling toll:{t['name']} with query:{t['args'].get('query','No query provided')}")
            
            if not t['name'] in tools_dict:
                result="Incorrect tool name,please Retry and select tool from available tools."
            else:
                result=tools_dict[t['name']].invoke(t['args'].get('query',''))
            results.append(ToolMessage(tool_call_id=t['id'],tool_name=t['name'],content=str(result)))
        return {"messages":results}

    ## 4.åˆ¤æ–·æ˜¯å¦ç¹¼çºŒæŸ¥è©¢çš„ation
    def should_continue(state: AgentState) -> bool:
        last_message=state["messages"][-1]
        return hasattr(last_message, "tool_calls") and len(last_message.tool_calls)>0
    
    ## 5.å®šç¾©node å’Œedges
    rag_graph = StateGraph(AgentState)
    rag_graph.add_node("llm", call_llm)
    rag_graph.add_node("retriver_angent", retriver_action)
    rag_graph.add_edge(START, "llm")
    rag_graph.add_conditional_edges("llm", should_continue, {True: "retriver_angent", False: END})
    rag_graph.add_edge("retriver_angent", "llm")
    rag_app=rag_graph.compile()
```

## step7: åŸ·è¡Œ function
```python
def running_agent():
    print("\n========= RAG Agent =========")
    while True:
        user_input= input("What is your Question (or 'exit' to quit): ")
        if user_input.lower() in ['exit', 'quit']:
            break
        messages = [HumanMessage(content=user_input)]
        result = rag_app.invoke({"messages":messages})
        print('\n========= Agent Answer =========')
        print(result['messages'][-1].content)

if __name__ == "__main__":
    running_agent()

```
- åŸ·è¡Œ uv run main.py , å°±å¯ä»¥é–‹å§‹å•å•é¡Œäº†
  ![run aggent](/public/images/20251028-181310.png)
